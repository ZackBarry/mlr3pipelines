---
title: "Pipeline Examples"
author: "Florian Pfisterer"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Showcase: Basic Concepts: PipeOp and Graph}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  cache = FALSE,
  collapse = TRUE,
  comment = "#>"
)
set.seed(8008135)
```

This vignette showcases the general syntax and semantic behind `mlr3pipelines`, going from small to more involved examples.

## A simple Linear Pipeline

This example creates a linear pipeline, where the data is transformed using **PCA** before
a decision tree is trained.
The idea is thus, to first scale all input features before we rotate them using principal component analysis.
After this transformation, we train a simple Decision Tree learner for classification.

As exemplary data, we will use the "`iris`" classification task.
This object contains the famous iris dataset and some meta-information, such as the target variable.

```{r}
  library(mlr3)
  task = mlr_tasks$get("iris")
```

We quickly split our data into a train and a test set:
```{r}
  test.idx = sample(seq_len(task$nrow), 30)
  train.idx = setdiff(seq_len(task$nrow), test.idx)
  # Set task to only use train indexes
  task$row_roles$use = train.idx
```

A Pipeline (or `Graph`) contains multiple pipeline operators ("`PipeOp`"s), where each `PipeOp`
transforms the data when it flows through it.

For this usecase, we require 3 transformations:

- A `PipeOp` that scales the data
- A `PipeOp` that performs PCA
- A `PipeOp` that contains the **Decision Tree** learner

A list of `PipeOp`s implemented in `mlr3pipelines` can be obtained from the
`mlr_pipeops` dictionary using `mlr_pipeops$keys()`.

First we define the required `PipeOp`s:

```{r}
  library(mlr3pipelines)
  op1 = PipeOpScale$new()
  op2 = PipeOpPCA$new()
  op3 = PipeOpLearner$new(learner = mlr_learners$get("classif.rpart"))
```

We can quickly visualize what we want to achieve:

```{r, echo = FALSE, fig.width = 7}
  linear_pipeline = op1 %>>% op2 %>>% op3
  # And visualize our pipleline
  linear_pipeline$plot(html = TRUE)
```

### A quick glance into a PipeOp

In order to get a better understanding of what the respective PipeOps do,
we quickly look at one of them in detail:

The most important slots in a PipeOp are:

- `$train()`: A function used to train the PipeOp.
- `$predict()`: A function used to predict with the PipeOp.

The `$train()` and `$predict()` functions define the core functionality of our PipeOp.
In many cases, in order to not leak information from the training set into the test set it is imperative to treat train and test data separately. For this we require a `$train()` function that learns the appropriate transformations from the training set and a `$test()` function that applies the transformation on future data.

In the case of `PipeOpPCA` this means the following:

- `$train()` learns a rotation matrix from its input and saves this matrix to
  an additional slot, `$state`. It returns the rotated input data stored
  in a new `Task`.
- `$predict()` uses the rotation matrix stored in `$state` in order to rotate
  future, unseen data. It returns those in a new `Task`.

### Constructing the Pipeline

We can now connect the `PipeOp`s constructed earlier to a **Pipeline**.
We can do this using the `%>>%` operator.

```{r}
  linear_pipeline = op1 %>>% op2 %>>% op3
  # And visualize our pipleline
  linear_pipeline$plot(html = TRUE)
```

The result of this operation is a "`Graph`".
A `Graph` connects the input and output of each `PipeOp` to the following `PipeOp`.
This allows us to specify linear processing pipelines.
In this case, we connect the output of the **scaling** PipeOp to the input of the **PCA** PipeOp
and the output of the **PCA** PipeOp to the input of **PipeOpLearner**.

We can now train the `Graph` using the `iris` **Task**.

```{r}
  linear_pipeline$train(task)
```

When we now train the graph, the data flows through the graph as follows:

- The Task flows into the `PipeOpScale`. The `PipeOp` scales each column in the data
  contained in the Task and returns a new Task that contains the scaled data to its output.
- The scaled task flows into the `PipeOpPCA`. PCA transforms the data and returns a (possibly smaller)
  Task, that contains the transformed data.
- This transformed data then flows into the learner, in our case **classif.rpart**.
  It is then used to train the learner, and as a result saves a model that can be
  used to predict new data.

In order to predict on new data, we need to save the relevant transformations our data went through
while training. As a result, each `PipeOp` saves a state, where information requried to appropriately
transform future data is stored. In our case, this is **mean** and **standard deviation** of each column for `PipeOpScale`, the PCA rotation matrix for `PipeOpPCA` and the learned model for `PipeOpLearner`.


```{r}
  # predict on test.idx
  task$row_roles$use = test.idx
  linear_pipeline$predict(task)
```

### Using the Pipeline as a mlr3 learner

In most cases, we want to use the pipeline just like an
**mlr3** learner. In order to achieve this,
we simply construct a `GraphLearner` that contains the
pipeline we defined.

```{r}
  graph_lrn = GraphLearner$new(linear_pipeline)
```

For training:

```{r}
  graph_lrn$train(task, train.idx)
  graph_lrn$model
```

testing:

```{r}
  pred = graph_lrn$predict(task, test.idx)
  pred
```

and scoring:

```{r}
  pred$score()
```

This allows us to seamlessly integrate pipelines with the
whole [mlr3](https://mlr3.mlr-org.com) ecosystem, and thus for example resample or benchmark pipelines, tune its params, and many more things.

For additional information on `mlr3` please refer to the respective vignettes.

## Branching and Unbranching

Consider a scenario, in which there are different options for a given modeling step.
A researcher might, in an initial step want to try different options for this.
This can be done by specifying **branching** and **unbranching** operators.
In the long term, this also allows us to tune over the whole space, i.e. we learn which preprocessing option makes sense
for our data.

We will investigate the inner workings of such a Graph.
In this example we consider three options for preprocessing our data before we train a model.

In a first step we define the different preprocessing steps:
We will consider scaling the data, transforming it using PCA, or not transforming the data at all (using `PipeOpNULL`).
```r
op1 = PipeOpScale$new()
op2 = PipeOpPCA$new()
op3 = PipeOpNULL$new()
opts = gunion(list(op1, op2, op3))
```
Additionally, we create the learner `PipeOp`.

```Å•
polrn = PipeOpLearner$new(mlr_learners$get("rpart"))
```

In order to create a mental image of our Pipeline, we have to consider, how the data flows through our
computational graph. In case we simply want  to do scaling, our pipeline would look like this:

```r
op1 %>>% polrn
```
If we want to actively select one `PipeOp` in a set, we need to add a `PipeOp` that orchestrates the selection of
the following `PipeOp` using `PipeOpBranch`. After choosing one `PipeOp`, all not-selected operators simply recieve
a `NO_OP` object.
We can now collect the activated/deactivated `PipeOp`s again using `PipeOpUnBranch`.
This will become clearer in a short example.

### PipeOpBranch

We create a `PipeOpBranch` that let's us select one of
the following $3$ operators. We have to initialize
this `PipeOp` with the number of output branches.

```r
po_b = PipeOpBranch$new(3)
```

This can be connected to the different preprocessing features.

```r
g = po_b %>>% opts
```

### PipeOpUnbranch

In order to collect the different inputs again, we use the
`PipeOpUnbranch` operator. Again, we have to initialize this
operator with the number of inputs it expects.

```r
po_ub = PipeOpUnbranch$new(3)
```
```r
g = g %>>% po_ub
```

We can visualize the resulting graph before diving into a
more detailed explanation.

```r
g$plot(html = TRUE)
```


### Feature Engineering

New features can be added or computed from the task using `PipeOpMutate`.
The operator evaluates one or multiple expressions provided in an `alist`.
In this example, we compute some new features on top of the `iris` task and
add the to the data.

```r
pom = PipeOpMutate$new()
mutations = alist(
  Sepal.Sum = Sepal.Length + Sepal.Width,
  Petal.Sum = Petal.Length + Petal.Width,
  Sepal.Petal.Ratio = (Sepal.Length / Petal.Length)
)
pom$param_set$values$mutation = mutations
```

If outside data is required, we can make use of the `env` parameter and additionally provide
an environment, where the expressions should be evalutated (`env` defaults to `.GlobalEnv`).

## Ensembles

Leveraging the different operations available to connect `PipeOps` to very powerfull graphs.
This vignette introduces two such well-known graph structures, that allow us to enhance single learners
to more powerfull combinations.

Before we go into details, we split the task into train
and test indices.

```{r}
  library(mlr3)
  library(mlr3pipelines)
  task = mlr_tasks$get("iris")
  train.idx = sample(seq_len(task$nrow), 120)
  test.idx = setdiff(seq_len(task$nrow), train.idx)
```

### Bagging

We first examine Bagging introduced by Breimann (1994).
The basic idea is to create multiple predictors and then aggregate those to a single, more powerfull predictor.
> "... multiple versions are formed
> by making bootstrap replicates of the learning set
> and using these as new learning sets" (Breimann 1994)

Bagging then aggregates a set of predictors by averaging (regression) or majority vote (classification).
The idea behind bagging is, that a set of weak, but different predictors can be combined in order to arrive
at a single, better predictor.

We can achieve this by downsampling our data before training a learner, repeating this for say $10$ times and then performing a majority vote on the predictions.


First, we create a simple pipeline, that uses
`PipeOpSubsample` before a `PipeOpLearner` is trained:

```{r}
  single_pred = PipeOpSubsample$new() %>>%
    PipeOpLearner$new(mlr_learners$get("classif.rpart"))
```

We can now repeat this $10$ times using `greplicate`.

```{r}
  pred_set = greplicate(single_pred, 10L)
```

In order to visually inspect the resulting graph,
we can plot it:

```{r, fig.width=7.5}
  pred_set$plot(html = TRUE)
```

Afterwards we need to aggregate the 10 pipelines
to form a single model:

```{r}
  bagging = pred_set %>>%
    PipeOpMajorityVote$new(innum = 10L)
```

and plot again to see what happens:

```{r, fig.width=7.5}
  bagging$plot(html = TRUE)
```

This pipeline can again be used in conjunction with
`GraphLearner` in order for Bagging to be used like a
[mlr3::Learner].

```{r}
  baglrn = GraphLearner$new(bagging)
  baglrn$train(task, train.idx)
  baglrn$predict(task, test.idx)
```

In conjunction with different `Backends`, this can be a very powerful tool, as in cases, where the data does not fully fit in memory, we can easily just obtain a fraction of the data for each learner from a `DataBaseBackend` and then aggregate predictions from all learners.

### Stacking

Stacking is another technique that can improve model performance. The basic idea behind stacking is, that
using predictions from one model as features of a subsequent model can possibly improve performance.

A very simple possibility would be to train a
decision tree and use the predictions from this
model in conjunction with the original features in order to train an additional model on top.
The basic idea behind this is, that patterns a model detected in the data can be used by a higher level model, and thus result in a better performance.

In order to limit overfitting, we additionally do not predict on
the original predictions of the learner, but instead on out-of-bag predictions. This is automatically done by
`PipeOpLearnerCV`.

We first create a level 0 learner, which is used to
extract a lower level prediction.
We additionally `clone()` the learner object to obtain a copy
of the learner, and set a custom id for the `PipeOp`.

```{r}
  lrn = mlr_learners$get("classif.rpart")
  lrn_0 = PipeOpLearnerCV$new(lrn$clone())
  lrn_0$id = "rpart_cv"
```

Additionally, we use a `PipeOpNULL` in parallel to
the level 0 learner, in order to send the unchanged
Task to the next level.

```{r}
  level_0 = gunion(list(lrn_0, PipeOpNULL$new()))
```

Afterwards, we want to concatenate the predictions from
`PipeOpLearnerCV` and the original Task using `PipeOpFeatureUnion`.

```{r}
  combined = level_0 %>>% PipeOpFeatureUnion$new(2)
```

We can now train another learner on top of the combined
features.

```{r, fig.width=7.5}
  stack = combined %>>% PipeOpLearner$new(lrn$clone())
  stack$plot(html = TRUE)
```

```{r}
  stacklrn = GraphLearner$new(stack)
  stacklrn$train(task, train.idx)
  stacklrn$predict(task, test.idx)
```

In this vignette, we only showcase a very simple usecase for stacking.
In many real-world applications, stacking is done for multiple levels and on multiple representations of the dataset. On a lower level, different preprocessing methods can for example
be defined in conjunction with several learners. On a higher level, we can then combine
those predictions in order to form a very powerfull model.

### Multilevel Stacking

In order to showcase the power of `mlr3pipelines`, we will quickly introduce a more complicated
stacking example.

In this case, we train a **glmnet** and 2 different *rpart* models (some transform its inputs using `PipeOpPCA`) on our task in the level 0 and concatenate them with the original features (via `PipeOpNull`).
This is then passed on to level 1, where we copy the concatenated features
$3$ times and put this task into a **rpart** and a **glmnet** model.
Additionally, we also keep a version of the level 0 output (via `PipeOpNull`) and pass this on to level 2. In level 2 we simply concatenate all level 1 outputs and train a final decision tree.


```{r, eval = FALSE}
  library(mlr3learners)
  rprt = mlr_learners$get("classif.rpart")
  rprt$predict_type = "prob"
  glmn = mlr_learners$get("classif.glmnet")
  glmn$predict_type = "prob"

  #  Create Learner CV Operators
  lrn_0 = PipeOpLearnerCV$new(rprt, id = "rpart_cv_1")
  lrn_0$values$maxdepth = 5L
  lrn_1 = PipeOpPCA$new(id = "pca1") %>>% PipeOpLearnerCV$new(rprt, id = "rpart_cv_2")
  lrn_1$values$maxdepth = 1L
  lrn_2 = PipeOpPCA$new(id = "pca2") %>>% PipeOpLearnerCV$new(glmn)

  # Union them with a PipeOpNULL to keep original features
  level_0 = gunion(list(lrn_0, lrn_1,lrn_2, PipeOpNULL$new(id = "NULL1")))

  # Cbind the output 3 times, train 2 learners but also keep level
  # 0 predictions
  level_1 = level_0 %>>%
    PipeOpFeatureUnion$new(4) %>>%
    PipeOpCopy$new(3) %>>%
    gunion(list(
      PipeOpLearnerCV$new(rprt, id = "rpart_cv_l1"),
      PipeOpLearnerCV$new(glmn, id = "glmnt_cv_l1"),
      PipeOpNULL$new(id = "NULL_l1")
    ))

  # Cbind predicitions, train a final learner.
  level_2 = level_1 %>>%
    PipeOpFeatureUnion$new(3, id = "u2") %>>%
    PipeOpLearner$new(rprt,
      id = "rpart_l2")

  # Plot the resulting graph
  level_2$plot(html = TRUE)

  task = mlr_tasks$get("iris")
  lrn = GraphLearner$new(level_2)

  lrn$
    train(task, train.idx)$
    predict(task, test.idx)$
    score()
```


### Training on Data Subsets

In cases, where data is too big to fit into the machines memory, an often-used technique is
to split the data into several parts, train on each part of the data and afterwards aggregate the models.
In this example, we split our data into $4$ parts using `PipeOpChunk`.
Additionally, we create $4$ `PipeOpLearner` PipeOps, which are then trained on each split of the data.

```{r}
  chks = PipeOpChunk$new(4)
  lrns = greplicate(PipeOpLearner$new(mlr_learners$get("classif.rpart")), 4)
```

Afterwards we can use `PipeOpMajorityVote` to aggregate the predictions from a the 4 different models into a new one.
```{r}
  mjv = PipeOpMajorityVote$new(4)
```

We can now connect the different operators and visualize the full graph:

```{r, fig.width=7.5, fig.height = 9}
  pipeline = chks %>>% lrns %>>% mjv
  pipeline$plot(html = TRUE)
```

```{r}
  pipelrn = GraphLearner$new(pipeline)
  pipelrn$train(task, train.idx)$
    predict(task, train.idx)$
    score()
```
